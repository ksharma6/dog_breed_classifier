{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch, torchvision\n",
    "\n",
    "os.chdir(\"/home/kishen/documents/python_projects/stanford_dogs/\")\n",
    "from src.data import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in image data\n",
    "path = \"/home/kishen/documents/python_projects/stanford_dogs/data/Images\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=.55),\n",
    "    torchvision.transforms.Normalize(mean= [0.4760, 0.4517, 0.3907], \n",
    "                                     std= [0.2362, 0.2314, 0.2307])\n",
    "\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(root_directory=path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train_size, val_size = int(dataset.__len__() * .8) , int(dataset.__len__() * .2)\n",
    "\n",
    "ran_gen= torch.Generator().manual_seed(24)\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, \n",
    "                                                    [train_size, val_size],\n",
    "                                                     generator= ran_gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculcate train data stats\n",
    "# from src.modeling import calculate_channel_means, calculate_channel_sds\n",
    "\n",
    "# display(calculate_channel_means(train_data, 'image'))\n",
    "# #tensor([0.4760, 0.4517, 0.3907])\n",
    "\n",
    "# display(calculate_channel_sds(train_data, 'image'))\n",
    "# #tensor([0.2362, 0.2314, 0.2307])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda\n"
     ]
    }
   ],
   "source": [
    "#init models and move to gpu\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"available device: \" + device)\n",
    "\n",
    "# alexnet = alexnet()\n",
    "# resnet18 = resnet18()\n",
    "\n",
    "# vgg16 = vgg16()\n",
    "# vgg16.classifier[-1] = torch.nn.Linear(in_features= 4096,\n",
    "#                                        out_features = 120)\n",
    "# vgg16.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#init criterion and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer_vgg = optim.SGD(vgg16.parameters(), \n",
    "#                           lr = .001, \n",
    "#                           momentum= .9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.modeling import train_model\n",
    "\n",
    "# vgg16_dict = {}\n",
    "\n",
    "# vgg16_dict = train_model(model=vgg16, \n",
    "#                          dataloader = train_dataloader, \n",
    "#                          epochs = 1,\n",
    "#                          optimizer = optimizer_vgg,\n",
    "#                          criterion = criterion, \n",
    "#                          device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs = 1\n",
    "# acc_per_epoch = [0] * epochs\n",
    "# loss_per_epoch = [0] * epochs\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     run_loss = 0\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     correct = 0\n",
    "#     for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "#         imgs, labels = batch['image'].to(device), batch['idx_label'].to(device)\n",
    "        \n",
    "#         #zero grad\n",
    "#         optimizer_vgg.zero_grad()\n",
    "\n",
    "#         #make predictions and calculate loss\n",
    "#         preds = vgg16(imgs)\n",
    "#         loss = criterion(preds, labels)\n",
    "\n",
    "#         #calculate gradient and step\n",
    "#         loss.backward()\n",
    "#         optimizer_vgg.step()\n",
    "\n",
    "#         #calculate metrics for mini-batch\n",
    "#         run_loss += loss.item()\n",
    "#         correct += (torch.argmax(preds, dim = 1) == labels).float().sum()\n",
    "\n",
    "\n",
    "#     #calculate epoch stats\n",
    "#     print(\"Epoch {} Statistics:\".format(e + 1))\n",
    "#     print(\"=\" * 20)\n",
    "\n",
    "#     epoch_loss = run_loss / len(train_dataloader)\n",
    "#     loss_per_epoch[e] = epoch_loss\n",
    "#     print(\"epoch loss: \", str(round(epoch_loss, 3)))\n",
    "\n",
    "#     acc = correct / len(train_dataloader.dataset)\n",
    "#     acc_per_epoch[e] = acc\n",
    "    \n",
    "#     print(\"epoch acc: {:.2f}%\\n\".format(acc * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 is on the following device:  cuda\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(num_ftrs, 120)\n",
    "resnet18.to(device)\n",
    "print(\"resnet18 is on the following device: \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_resnet18 = optim.SGD(resnet18.parameters(), \n",
    "                          lr = .001, \n",
    "                          momentum= .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/stanford_dogs/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Statistics:\n",
      "====================\n",
      "epoch loss:  3.617\n",
      "epoch acc: 27.77%\n",
      "\n",
      "Epoch 2 Statistics:\n",
      "====================\n",
      "epoch loss:  1.89\n",
      "epoch acc: 63.34%\n",
      "\n",
      "Epoch 3 Statistics:\n",
      "====================\n",
      "epoch loss:  1.308\n",
      "epoch acc: 71.94%\n",
      "\n",
      "Epoch 4 Statistics:\n",
      "====================\n",
      "epoch loss:  1.034\n",
      "epoch acc: 76.63%\n",
      "\n",
      "Epoch 5 Statistics:\n",
      "====================\n",
      "epoch loss:  0.875\n",
      "epoch acc: 79.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.modeling import train_model\n",
    "\n",
    "resnet18_dict = {}\n",
    "\n",
    "resnet18_dict = train_model(model=resnet18, \n",
    "                         dataloader = train_dataloader, \n",
    "                         epochs = 5,\n",
    "                         optimizer = optimizer_resnet18,\n",
    "                         criterion = criterion, \n",
    "                         device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AlexNet' object has no attribute 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m alexnet \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39malexnet()\n\u001b[0;32m----> 3\u001b[0m alexnet\u001b[39m.\u001b[39;49mfc\n\u001b[1;32m      4\u001b[0m \u001b[39m# resnet18.fc = torch.nn.Linear(num_ftrs, 120)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# resnet18.to(device)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# print(\"resnet18 is on the following device: \", device)\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/stanford_dogs/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AlexNet' object has no attribute 'fc'"
     ]
    }
   ],
   "source": [
    "alexnet = models.alexnet()\n",
    "\n",
    "alexnet.fc\n",
    "# resnet18.fc = torch.nn.Linear(num_ftrs, 120)\n",
    "# resnet18.to(device)\n",
    "# print(\"resnet18 is on the following device: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 is on the following device:  cuda\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(num_ftrs, 120)\n",
    "resnet18.to(device)\n",
    "print(\"resnet18 is on the following device: \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_resnet18 = optim.SGD(resnet18.parameters(), \n",
    "                          lr = .001, \n",
    "                          momentum= .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/stanford_dogs/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Statistics:\n",
      "====================\n",
      "epoch loss:  3.617\n",
      "epoch acc: 27.77%\n",
      "\n",
      "Epoch 2 Statistics:\n",
      "====================\n",
      "epoch loss:  1.89\n",
      "epoch acc: 63.34%\n",
      "\n",
      "Epoch 3 Statistics:\n",
      "====================\n",
      "epoch loss:  1.308\n",
      "epoch acc: 71.94%\n",
      "\n",
      "Epoch 4 Statistics:\n",
      "====================\n",
      "epoch loss:  1.034\n",
      "epoch acc: 76.63%\n",
      "\n",
      "Epoch 5 Statistics:\n",
      "====================\n",
      "epoch loss:  0.875\n",
      "epoch acc: 79.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.modeling import train_model\n",
    "\n",
    "resnet18_dict = {}\n",
    "\n",
    "resnet18_dict = train_model(model=resnet18, \n",
    "                         dataloader = train_dataloader, \n",
    "                         epochs = 5,\n",
    "                         optimizer = optimizer_resnet18,\n",
    "                         criterion = criterion, \n",
    "                         device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford_dogs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
