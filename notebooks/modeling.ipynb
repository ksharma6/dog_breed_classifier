{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch, torchvision\n",
    "\n",
    "os.chdir(\"/home/kishen/documents/python_projects/stanford_dogs/\")\n",
    "from src.data import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in image data\n",
    "path = \"/home/kishen/documents/python_projects/stanford_dogs/data/Images\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=.55),\n",
    "    torchvision.transforms.Normalize(mean= [0.4760, 0.4517, 0.3907], \n",
    "                                     std= [0.2362, 0.2314, 0.2307])\n",
    "\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(root_directory=path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train_size, val_size = int(dataset.__len__() * .8) , int(dataset.__len__() * .2)\n",
    "\n",
    "ran_gen= torch.Generator().manual_seed(24)\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, \n",
    "                                                    [train_size, val_size],\n",
    "                                                     generator= ran_gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculcate train data stats\n",
    "# from src.modeling import calculate_channel_means, calculate_channel_sds\n",
    "\n",
    "# display(calculate_channel_means(train_data, 'image'))\n",
    "# #tensor([0.4760, 0.4517, 0.3907])\n",
    "\n",
    "# display(calculate_channel_sds(train_data, 'image'))\n",
    "# #tensor([0.2362, 0.2314, 0.2307])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We will evaluate performances using EfficientNet, Resnet18, and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda\n"
     ]
    }
   ],
   "source": [
    "#init models, update FC layers, and move models to gpu\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"available device: \" + device)\n",
    "\n",
    "efficientNet = models.efficientnet_b0()\n",
    "\n",
    "#resnet18\n",
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(num_ftrs, 120)\n",
    "resnet18.to(device)\n",
    "\n",
    "\n",
    "vgg16 = models.vgg16()\n",
    "\n",
    "# vgg16.classifier[-1] = torch.nn.Linear(in_features= 4096,\n",
    "#                                        out_features = 120)\n",
    "# vgg16.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init criterion and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNet = models.efficientnet_b0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 is on the following device:  cuda\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(num_ftrs, 120)\n",
    "resnet18.to(device)\n",
    "print(\"resnet18 is on the following device: \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_resnet18 = optim.SGD(resnet18.parameters(), \n",
    "                          lr = .001, \n",
    "                          momentum= .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/stanford_dogs/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Statistics:\n",
      "====================\n",
      "epoch loss:  3.631\n",
      "epoch acc: 28.15%\n",
      "\n",
      "Epoch 2 Statistics:\n",
      "====================\n",
      "epoch loss:  1.896\n",
      "epoch acc: 63.62%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.modeling import train_model\n",
    "\n",
    "resnet18_dict = {}\n",
    "\n",
    "resnet18_dict = train_model(model=resnet18, \n",
    "                         dataloader = train_dataloader, \n",
    "                         epochs = 5,\n",
    "                         optimizer = optimizer_resnet18,\n",
    "                         criterion = criterion, \n",
    "                         device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16()\n",
    "\n",
    "# vgg16.classifier[-1] = torch.nn.Linear(in_features= 4096,\n",
    "#                                        out_features = 120)\n",
    "# vgg16.to(device)\n",
    "\n",
    "\n",
    "#init criterion and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer_vgg = optim.SGD(vgg16.parameters(), \n",
    "#                           lr = .001, \n",
    "#                           momentum= .9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.modeling import train_model\n",
    "\n",
    "# vgg16_dict = {}\n",
    "\n",
    "# vgg16_dict = train_model(model=vgg16, \n",
    "#                          dataloader = train_dataloader, \n",
    "#                          epochs = 1,\n",
    "#                          optimizer = optimizer_vgg,\n",
    "#                          criterion = criterion, \n",
    "#                          device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford_dogs",
   "language": "python",
   "name": "stanford_dogs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
