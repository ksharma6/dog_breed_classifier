{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch, torchvision\n",
    "\n",
    "os.chdir(\"/home/kishen/documents/python_projects/stanford_dogs\")\n",
    "\n",
    "from src.data import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data and split to train/test sets\n",
    "root = \"/home/kishen/documents/python_projects/stanford_dogs/data\"\n",
    "X = ImageDataset(root)\n",
    "\n",
    "train_size = int(X.__len__() * .8)\n",
    "test_size = X.__len__() - train_size\n",
    "\n",
    "ran_gen= torch.Generator().manual_seed(24)\n",
    "train_set, val_set = torch.utils.data.random_split(X, \n",
    "                                                  [train_size, test_size],\n",
    "                                                  generator= ran_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculcate trainig set metrics\n",
    "from src.modeling import calculate_channel_means, calculate_channel_sds\n",
    "\n",
    "# train_mean = calculate_channel_means(train_set, \"image\")\n",
    "# train_sd = calculate_channel_sds(train_set, 'image')\n",
    "\n",
    "# display(train_mean) #tensor([121.4168, 115.1198,  99.5976])\n",
    "# display(train_sd) #tensor([61.0905, 59.8789, 59.7863])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append transformation to train + val sets\n",
    "path = \"/home/kishen/documents/python_projects/stanford_dogs/data\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    \n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    #torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean= [121.3803, 115.1846,  99.6223], \n",
    "                                     std= [61.1698, 59.9492, 59.7626])\n",
    "\n",
    "])\n",
    "\n",
    "train_set.transform = transform\n",
    "val_set.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    Normalize(mean=[121.3803, 115.1846, 99.6223], std=[61.1698, 59.9492, 59.7626])\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishen/.virtualenvs/stanford_dogs/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0200e+00, -1.1135e+00, -9.2370e-01,  ..., -5.0265e-01,\n",
       "           5.9349e-02, -5.1616e-01],\n",
       "         [-3.6611e-01, -5.3133e-01, -6.8775e-01,  ...,  2.8792e-02,\n",
       "           2.5074e-01,  1.7443e-01],\n",
       "         [ 1.7513e-01, -1.3195e-01, -5.9421e-01,  ...,  1.6390e-01,\n",
       "           4.9874e-01,  5.9645e-01],\n",
       "         ...,\n",
       "         [ 6.4050e-01,  6.5587e-01,  2.6301e-01,  ...,  9.6822e-02,\n",
       "          -5.2876e-01, -3.5771e-01],\n",
       "         [ 8.2906e-01,  5.9619e-01,  2.2153e-01,  ...,  6.8766e-02,\n",
       "          -2.0991e-01, -1.2618e-01],\n",
       "         [ 3.3419e-01, -2.9611e-02, -1.2232e-01,  ..., -1.0677e-01,\n",
       "           2.0318e-04,  4.7686e-02]],\n",
       "\n",
       "        [[-9.8750e-01, -1.1158e+00, -9.6941e-01,  ..., -2.1095e-01,\n",
       "           2.9022e-01, -2.6010e-01],\n",
       "         [-3.3694e-01, -5.1390e-01, -7.0694e-01,  ...,  1.9604e-01,\n",
       "           4.0452e-01,  3.6488e-01],\n",
       "         [ 2.6537e-01, -9.8010e-02, -6.0305e-01,  ...,  2.3179e-01,\n",
       "           5.3360e-01,  7.2629e-01],\n",
       "         ...,\n",
       "         [ 9.6660e-01,  1.0847e+00,  8.4272e-01,  ...,  6.6799e-01,\n",
       "           2.5937e-01,  2.6670e-01],\n",
       "         [ 9.9448e-01,  9.0342e-01,  7.4396e-01,  ...,  8.5743e-01,\n",
       "           4.9783e-01,  5.2847e-01],\n",
       "         [ 4.9438e-01,  2.0658e-01,  2.7554e-01,  ...,  3.4495e-01,\n",
       "           3.9576e-01,  3.6240e-01]],\n",
       "\n",
       "        [[-8.1304e-01, -9.2420e-01, -7.5743e-01,  ..., -2.1410e-01,\n",
       "           4.4956e-01,  3.6978e-02],\n",
       "         [-1.2779e-01, -3.0530e-01, -4.9054e-01,  ...,  2.9211e-01,\n",
       "           4.8107e-01,  4.7598e-01],\n",
       "         [ 4.9313e-01,  1.4535e-01, -3.4453e-01,  ...,  1.1971e-01,\n",
       "           4.7941e-01,  7.5392e-01],\n",
       "         ...,\n",
       "         [ 1.0003e+00,  1.0187e+00,  5.9353e-01,  ...,  3.6928e-01,\n",
       "          -1.4173e-01, -1.2147e-01],\n",
       "         [ 1.1349e+00,  8.9657e-01,  5.0835e-01,  ...,  6.0186e-01,\n",
       "           2.7468e-01,  3.0071e-01],\n",
       "         [ 6.2563e-01,  2.5090e-01,  1.7398e-01,  ...,  2.9538e-01,\n",
       "           3.5708e-01,  3.5908e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(401)['image']\n",
    "\n",
    "root = \"/home/kishen/documents/python_projects/stanford_dogs/data\"\n",
    "test = ImageDataset(root, transform=transform)\n",
    "\n",
    "test.__getitem__(401)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kishen/documents/python_projects/stanford_dogs/notebooks/modeling.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kishen/documents/python_projects/stanford_dogs/notebooks/modeling.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#create dataloaders\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kishen/documents/python_projects/stanford_dogs/notebooks/modeling.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kishen/documents/python_projects/stanford_dogs/notebooks/modeling.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kishen/documents/python_projects/stanford_dogs/notebooks/modeling.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m val_dataloader \u001b[39m=\u001b[39m DataLoader(val_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "#create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculcate train data stats\n",
    "# from src.modeling import calculate_channel_means, calculate_channel_sds\n",
    "\n",
    "# display(calculate_channel_means(train_data, 'image'))\n",
    "# #tensor([0.4760, 0.4517, 0.3907])\n",
    "\n",
    "# display(calculate_channel_sds(train_data, 'image'))\n",
    "# #tensor([0.2362, 0.2314, 0.2307])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We will evaluate performances using EfficientNet, Resnet18, and VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init device, criterion, training loop, num_classes\n",
    "from src.modeling import train_model, model_check_cuda, plot_loss, plot_acc\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device is:\", device )\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_classses = len(dataset.class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init efficientNet and move to Device\n",
    "efficientNet = torchvision.models.efficientnet_b0()\n",
    "print(efficientNet.classifier[-1])\n",
    "\n",
    "efficientNet.classifier[-1] = torch.nn.Linear(in_features= 1280, out_features = num_classses)\n",
    "\n",
    "efficientNet.to(device)\n",
    "model_check_cuda(efficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init optimizer and train hyperparams\n",
    "opt_params = {#\"params\": None,\n",
    "              \"lr\":.001,\n",
    "              \"momentum\": .9}\n",
    "\n",
    "train_params = {#'model': None,\n",
    "                'dataloader':train_dataloader, \n",
    "                'epochs':10,\n",
    "                #'optimizer': None,\n",
    "                'criterion':criterion, \n",
    "                'device':device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train efficientNet\n",
    "# import torch.optim as optim\n",
    "\n",
    "# opt_effNet = optim.SGD(params= efficientNet.parameters(), **opt_params)\n",
    "\n",
    "# efficientNet_dict = {}\n",
    "\n",
    "# efficientNet_dict = train_model(model= efficientNet,\n",
    "#                                 optimizer= opt_effNet,\n",
    "#                                 **train_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_acc(efficientNet_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford_dogs",
   "language": "python",
   "name": "stanford_dogs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
